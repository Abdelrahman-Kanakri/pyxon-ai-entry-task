# RAG Techniques Comparison: Architecture Decision Documentation

## Executive Summary

This document explains the technical rationale behind choosing **Hybrid Retrieval** over **Graph RAG** and **RAPTOR** for this project. All three techniques have their merits, but given the project constraints (time, scope, Arabic language requirements), Hybrid Retrieval was the optimal choice.

---

## The Three Techniques Explained

### 1. Hybrid Retrieval (‚úÖ Implemented)

**What it is:** Combines semantic search (vector embeddings) with keyword-based search (BM25/TF-IDF), then uses a reranker to merge and prioritize results.

```
Query ‚Üí [Vector Search] ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚Üí [Reranker] ‚Üí Final Results
      ‚Üí [Keyword Search] ‚îÄ‚îò
```

**How we implemented it:**

- **Semantic Search:** ChromaDB with `multilingual-e5-small` embeddings
- **Keyword Search:** SQL-based full-text search on chunk content
- **Reranking:** Cross-encoder model for result fusion

---

### 2. Graph RAG (‚ùå Not Implemented)

**What it is:** Builds a knowledge graph from documents where entities become nodes and relationships become edges. Retrieval traverses the graph to find contextually connected information.

```
Documents ‚Üí [Entity Extraction] ‚Üí [Relation Extraction] ‚Üí Knowledge Graph
                                                              ‚Üì
Query ‚Üí [Entity Linking] ‚Üí [Graph Traversal] ‚Üí Subgraph ‚Üí LLM Answer
```

**Key components:**

- Named Entity Recognition (NER)
- Relation extraction
- Graph database (Neo4j, Amazon Neptune)
- Graph query language (Cypher, SPARQL)

---

### 3. RAPTOR (‚ùå Not Implemented)

**What it is:** Recursive Abstractive Processing for Tree-Organized Retrieval. Builds a hierarchical tree of document summaries at multiple abstraction levels.

```
Original Chunks (Level 0)
        ‚Üì [Cluster & Summarize]
Summary Chunks (Level 1)
        ‚Üì [Cluster & Summarize]
Higher-Level Summaries (Level 2)
        ‚Üì
...continues until single root summary
```

**Key components:**

- Clustering algorithm (usually UMAP + GMM)
- LLM for abstractive summarization
- Tree-structured index
- Multi-level retrieval

---

## Detailed Comparison Table

| Aspect                   | Hybrid Retrieval   | Graph RAG                | RAPTOR                                           |
| ------------------------ | ------------------ | ------------------------ | ------------------------------------------------ |
| **Implementation Time**  | 1-2 days           | 4-7 days                 | 3-5 days                                         |
| **Complexity**           | Moderate           | Very High                | High                                             |
| **Infrastructure**       | Vector DB + SQL    | Vector DB + Graph DB     | Vector DB + Tree Store                           |
| **LLM Calls (Indexing)** | 0                  | 0-Few                    | Many (per summary level)                         |
| **LLM Calls (Query)**    | 1                  | 1                        | 1                                                |
| **Best For**             | General documents  | Entity-rich documents    | Long documents needing multi-level understanding |
| **Arabic Support**       | Excellent          | Limited (NER challenges) | Moderate (summarization quality)                 |
| **Query Types**          | Semantic + Keyword | Entity relationships     | Hierarchical/thematic                            |

---

## Why Hybrid Retrieval Was Chosen

### ‚úÖ Advantages of Hybrid Retrieval

| Advantage                         | Explanation                                                                                                   |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| **Balanced Recall & Precision**   | Semantic search catches paraphrased content; keyword search catches exact terms (names, IDs, technical terms) |
| **Low Implementation Complexity** | Uses existing infrastructure (ChromaDB + SQL) without additional databases                                    |
| **Excellent Arabic Support**      | `multilingual-e5-small` handles Arabic embeddings well; keyword search works natively with Arabic text        |
| **No Additional LLM Costs**       | No LLM calls required during indexing                                                                         |
| **Fast Query Times**              | ~350ms average (as benchmarked)                                                                               |
| **Production Ready**              | Well-established pattern with predictable behavior                                                            |
| **Graceful Degradation**          | If one retrieval method fails, the other still provides results                                               |

### ‚ùå Disadvantages of Hybrid Retrieval

| Disadvantage                         | Mitigation in Our Implementation                      |
| ------------------------------------ | ----------------------------------------------------- |
| **No explicit entity relationships** | Cross-encoder reranking helps surface related content |
| **No hierarchical understanding**    | Dynamic chunking preserves semantic boundaries        |
| **May miss multi-hop reasoning**     | LLM synthesizes connections in the generation phase   |

---

## Why Graph RAG Was NOT Chosen

### ‚ùå Challenges That Made Graph RAG Unsuitable

| Challenge                          | Details                                                                                                                                                          |
| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Arabic NER is Limited**          | Arabic Named Entity Recognition models (CAMeLBERT-NER) have lower accuracy than English models. Would require custom training data for domain-specific entities. |
| **Relation Extraction for Arabic** | Very few pre-trained models exist for Arabic relation extraction. Would need custom development.                                                                 |
| **Additional Infrastructure**      | Requires graph database (Neo4j/Amazon Neptune) alongside existing Vector DB and SQL DB.                                                                          |
| **Schema Design**                  | Requires domain ontology design‚Äîwhat entity types exist? What relationships? This is time-consuming.                                                             |
| **Query Translation**              | Converting natural language queries to Cypher/SPARQL is error-prone without fine-tuning.                                                                         |
| **Time Constraints**               | Estimated 4-7 days for basic implementation vs. 5-day project deadline.                                                                                          |

### ‚úÖ When Graph RAG Would Be Better

- **Entity-centric documents:** Legal contracts, medical records, organizational charts
- **Known entity types:** When you know exactly what entities to extract (people, organizations, dates)
- **Multi-hop queries:** "Who is the manager of the person who wrote document X?"
- **Mature English-only systems:** Where NER models are well-established

### üìä Graph RAG Pros/Cons Summary

| Pros                               | Cons                                   |
| ---------------------------------- | -------------------------------------- |
| Excellent for relationship queries | High implementation complexity         |
| Handles multi-hop reasoning        | Requires graph database infrastructure |
| Explicit entity connections        | Arabic NER is immature                 |
| Reduces hallucinations about facts | Schema design overhead                 |

---

## Why RAPTOR Was NOT Chosen

### ‚ùå Challenges That Made RAPTOR Unsuitable

| Challenge                         | Details                                                                                                     |
| --------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **High LLM Cost During Indexing** | Each document requires multiple summarization calls per level. A 10-page document might need 20+ LLM calls. |
| **Arabic Summarization Quality**  | Most LLMs produce lower-quality abstractive summaries in Arabic. Could lose critical nuances.               |
| **Latency During Indexing**       | Building the tree is slow due to sequential LLM calls.                                                      |
| **Complexity of Tree Management** | Need to handle tree updates when documents change.                                                          |
| **Overkill for Short Documents**  | RAPTOR excels for very long documents (books, reports). Many of our test cases are standard-length.         |
| **Clustering Quality**            | UMAP + GMM clustering may not work optimally for short or mixed-language chunks.                            |

### ‚úÖ When RAPTOR Would Be Better

- **Very long documents:** Books, research papers, legal documents (100+ pages)
- **Thematic queries:** "What are the main themes in this book?"
- **Summarization-first use cases:** When users often ask for overviews before details
- **English-dominant content:** Where LLM summarization quality is highest

### üìä RAPTOR Pros/Cons Summary

| Pros                           | Cons                                      |
| ------------------------------ | ----------------------------------------- |
| Excellent for long documents   | High LLM cost during indexing             |
| Captures hierarchical themes   | Arabic summarization quality concerns     |
| Good for "big picture" queries | Slow indexing due to sequential LLM calls |
| Reduces chunk fragmentation    | Overkill for short/medium documents       |

---

## Implementation Feasibility Analysis

### Time Breakdown Estimate

| Technique        | Core Implementation | Arabic Adaptation | Testing & Debugging | Total           |
| ---------------- | ------------------- | ----------------- | ------------------- | --------------- |
| Hybrid Retrieval | 4-8 hours           | 2-3 hours         | 4-6 hours           | **10-17 hours** |
| Graph RAG        | 16-24 hours         | 16-24 hours       | 8-16 hours          | **40-64 hours** |
| RAPTOR           | 12-16 hours         | 8-12 hours        | 6-10 hours          | **26-38 hours** |

Given a **~5-day deadline** with other core requirements (document parsing, Arabic support, benchmarking, demo), only Hybrid Retrieval was feasible to implement properly.

---

## Technical Deep Dive: Our Hybrid Retrieval Implementation

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Query Input                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Semantic Search      ‚îÇ     ‚îÇ    Keyword Search       ‚îÇ
‚îÇ    (ChromaDB)           ‚îÇ     ‚îÇ    (SQL Full-Text)      ‚îÇ
‚îÇ                         ‚îÇ     ‚îÇ                         ‚îÇ
‚îÇ ‚Ä¢ multilingual-e5-small ‚îÇ     ‚îÇ ‚Ä¢ BM25-like scoring     ‚îÇ
‚îÇ ‚Ä¢ Cosine similarity     ‚îÇ     ‚îÇ ‚Ä¢ Exact term matching   ‚îÇ
‚îÇ ‚Ä¢ Top-k retrieval       ‚îÇ     ‚îÇ ‚Ä¢ Top-k retrieval       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                               ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ    Result Fusion        ‚îÇ
              ‚îÇ    (RRF or Weighted)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ    Cross-Encoder        ‚îÇ
              ‚îÇ    Reranker             ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ    Top-k Final Results  ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why This Works Well for Arabic

1. **Embedding Model:** `multilingual-e5-small` was trained on 100+ languages including Arabic
2. **Keyword Fallback:** When semantic search misses exact Arabic terms (especially with diacritics), keyword search catches them
3. **Cross-Encoder:** Validates relevance using contextual understanding

---

## Addressing Common Interview Questions

### Q1: "Why didn't you implement Graph RAG if it was recommended?"

**Answer:**

> "Graph RAG was listed as 'consider implementing'‚Äîan optional enhancement, not a requirement. The main challenge was Arabic NER. Arabic Named Entity Recognition models have significantly lower accuracy than English models, and domain-specific entities would require custom training data. Given the 5-day deadline, I prioritized delivering a complete, working solution with Hybrid Retrieval, which addresses the core retrieval quality concerns while remaining feasible within the timeframe. Graph RAG is documented as a future enhancement."

### Q2: "Wouldn't RAPTOR have given better results for long documents?"

**Answer:**

> "RAPTOR excels for very long documents like books or extensive reports. However, it requires multiple LLM calls per document during indexing, which adds significant cost and latency. More importantly, Arabic abstractive summarization quality in current LLMs is lower than English, risking information loss in the hierarchy. For our use case with mixed-length documents, the semantic-aware dynamic chunking combined with hybrid retrieval provides excellent results without the indexing overhead."

### Q3: "How does Hybrid Retrieval compare to Graph RAG for complex queries?"

**Answer:**

> "Graph RAG excels at explicit relationship queries like 'Who manages the person who wrote document X?' because relationships are pre-extracted and stored. Hybrid Retrieval handles these through the LLM synthesis phase‚Äîwe retrieve relevant chunks, and the LLM connects the dots. For most practical RAG queries (finding relevant information, Q&A), Hybrid Retrieval performs comparably with much lower implementation complexity. Our benchmark shows 100% hit rate and MRR of 1.0, demonstrating that retrieval quality is not compromised."

### Q4: "What would make you reconsider and implement Graph RAG?"

**Answer:**

> "Three conditions would make Graph RAG the right choice:
>
> 1. **Stable entity schema:** If we knew the exact entity types (e.g., employees, projects, departments)
> 2. **English-only or high-quality Arabic NER:** If Arabic NER models improve or we have labeled training data
> 3. **Relationship-centric queries:** If users frequently ask about connections between entities rather than content search
>
> In future iterations, Graph RAG could be added alongside Hybrid Retrieval for entity-specific queries."

---

## Summary Decision Matrix

| Criterion                    | Hybrid | Graph RAG | RAPTOR | Winner        |
| ---------------------------- | ------ | --------- | ------ | ------------- |
| Implementation within 5 days | ‚úÖ     | ‚ùå        | ‚ö†Ô∏è     | Hybrid        |
| Arabic language support      | ‚úÖ     | ‚ö†Ô∏è        | ‚ö†Ô∏è     | Hybrid        |
| No additional infrastructure | ‚úÖ     | ‚ùå        | ‚úÖ     | Hybrid/RAPTOR |
| No indexing LLM costs        | ‚úÖ     | ‚úÖ        | ‚ùå     | Hybrid/Graph  |
| Retrieval accuracy (general) | ‚úÖ     | ‚úÖ        | ‚úÖ     | Tie           |
| Entity relationships         | ‚ö†Ô∏è     | ‚úÖ        | ‚ö†Ô∏è     | Graph RAG     |
| Long document handling       | ‚ö†Ô∏è     | ‚ö†Ô∏è        | ‚úÖ     | RAPTOR        |

**Final Choice: Hybrid Retrieval** ‚Äî Best balance of capability, feasibility, and Arabic support.

---

## Conclusion

The decision to implement Hybrid Retrieval was based on:

1. **Requirement analysis:** It was one of the three "consider implementing" options‚Äîwe implemented it fully
2. **Arabic support priority:** Critical requirement that eliminates Graph RAG (poor NER) and penalizes RAPTOR (summarization quality)
3. **Time feasibility:** Only realistic option within the deadline
4. **Benchmark results:** 100% hit rate, MRR 1.0 demonstrates the approach works
5. **Future extensibility:** Graph RAG and RAPTOR are documented as enhancements, not rejected

This demonstrates pragmatic engineering decision-making: deliver a complete, working solution that meets requirements rather than an incomplete implementation of a more advanced technique.
